> 本章内容包括：
> - 保持pod的健康
> - 运行同一个pod的多个实例
> - 在节点异常之后自动重新调度pod
> - 水平缩放pod
> - 在集群节点上运行系统级的pod
> - 运行批量任务
> - 调度任务定时执行或在未来执行一次

在上一节中，学会了如何创建、监控和管理pod，但在实际应用中，我们希望部署能自动保持运行，并且保持健康，无需手动干预任何东西，而要做到以上几点，我们需要创建`ReplicationController` 或`Deployment`这类资源，由他们来创建并管理实际的pod。
# 保持pod健康
在Kubernetes中，只要将pod调度到某个节点上，该节点上的kubelet就会运行pod的容器，从此只要该pod存在，就会保持运行。如果容器的主进程崩溃，那么kubelet将重启容器。如果pod中运行的应用程序有一个隔一段时间就会导致该程序崩溃的bug，Kubernetes会自动重启应用程序，所以即使该应用程序本身没有做任何特殊的事情，在Kubernetes中运行也会自动获得自我修复的能力。

Kubernetes会保证把崩溃的pod自动重启，pod的状态直接由pod中运行的主容器决定，主容器状态又由容器内运行的应用程序决定，所以应用程序是否能在无法响应的时候被Kubernetes重启，最主要的就是Kubernetes能准确判断应用程序的状态。

即使进程没有崩溃，有时应用程序也会停止正常工作。例如，具有内存泄漏的java应用程序抛出`OOM`，但JVM进程会一直运行，导致容器无法正常在应用程序无响应时退出。所以为确保这种应用程序能正常退出，必须从外部检查应用程序的运行状况，而不是依赖于应用的内部监测。
## 存活探针
Kubernetes可以通过存活探针(liveness probe)检查容器是否还在运行。可以为pod中的每个容器单独指定存活探针。Kubernetes会定期执行探针，如果探测失败，则重新启动容器。

Kubernetes有以下三种探测容器的机制：
- HTTP GET探针对容器的IP地址(指定的端口和路径)执行HTTP GET请求。如果探针收到响应并且响应的状态码不代表错误，则认为容器状态是正常的。如果容器返回错误响应码或根本没有响应，则认为容器状态异常，容器将被重新启动。
- TCP套接字探针尝试与容器指定端口建立TCP连接。如果连接成功建立，则代表容器状态正常，否则重启容器。
- Exec探针在容器内执行命令，并检查命令的退出状态码。如果状态码为0，标识容器正常，否则重启。

### HTTP存活探针
来试着为一个Web应用添加一个存活探针来检查Web服务器是否能提供请求。
```kubernetes
apiVeersion: v1
kind: Pod
metadata:
	name: kubia-liveness
spec:
	containers:
	- image: luksa/kubia-unhealthy
	  name: kubia
	  livenessProbe:
		httpGet:   # HttpGet存活探针
		    path: /  # 请求路径
		    port: 8080  # 端口
		initialDelaySeconds: 3 # 等待初始化时间
		periodSeconds: 5  # 间隔时间
		failureThreshold：3 # 连续失败的次数，默认3
		timeoutSeconds：1 # 探针执行超时的时间，默认1秒
```
该pod的描述文件定义了一个httpGet存活探针，该探针告诉Kubernetes定期5秒在指定路径上执行http Get请求，以确定容器是否健康，该请求在容器运行3秒后开始执行。

### 使用存活探针
创建该pod后，可以使用`kubectl get pod`查看信息
```bash
[root@master k]# kubectl get po
NAME                             READY   STATUS    RESTARTS   AGE
kubia-liveness                   1/1     Running   1          31m
```
RESTARTS列显示已被重启了一次。

重启容器其实会创建一个新的pod，而不是在原来pod中重启容器。所以如果想查看重启之前的容器日志，可以使用`kubectl logs kubia-liveness --previous` 查看。

还可以使用`kubectl describe po/kubia-liveness`查看详细信息。
```bash
[root@master k]# kubectl describe po/kubia-livess
Last State: Terminated
	Reason: Error
	Exit COde: 137
.....
Restart Count: 1  # 重启一次
Liveness:     http-get http://:8080/ delay=3s timeout=1s period=5s #success=1 #failure=3  # 存活探针信息
```
可以看到之前的容器由于错误而终止，退出代码为137，这个代码有特殊含义，表示该进程由外部信号终止。数字137是128+x，x就是终止进程的信号编号。这里表示该进程由信号9终止，也就是`SGKILL`。

### 如何有效的创建探针
在生产环境中运行的pod，一定要定义一个存活探针。没有定义探针，kubernetes将无法确认你的应用是否健康，只要进程还在继续执行，那么kubernetes会认为容器是健康的。

**存活探针应该检查什么**
简单的存活探针仅仅只检查服务器是否响应，但即使是这样，也能让kubernetes快速的确认应用是否健康。为了更好的进行存活检查，可以将探针监测配置指定的URL路径(如/health)，并让应用从内部对内部运行的重要组件进行状态检查，以确保它们都没有终止或停止响应。

在设置`/health`路径时，不要设置身份认证，否则探针会一直探测失败，让容器无限重启。

确保内部检查仅仅只检查应内部原因而导致的问题，确保不要有任何外部因素。如后端数据库提供服务异常不应该视为应用异常，因为重启应用容器不会解决任何文艺，且在后端数据库恢复正常之前，容器会无限重启，导致浪费资源。

**保持探针轻量**
存活探针不应消耗太多的资源，并且运行的时间不会太长。默认情况下，探针的执行频率较高，必须在一秒之内执行完毕。一个过重的探针会大大拖慢容器的运行。且探针的CPU时间也会计入容器的CPU时间配额，所以一个过重的探针会减少主程序进程可用的CPU时间。

如果在容器中运行Java程序，请确保使用Http GET探针，而不是需要把启动全新的JVM获取存活信息的EXEC探针。JVM启动过程会占用大量计算资源。

**无需在探针中实现重试循环**
探针的失败阈值是可以通过`failureThreshold`进行配置的，并且在容器重启之前探针必须重试多次。但即使阈值设置为1，kubernetes为确认一次探针的失败，往往会重试多次，所以不需要在探针中自己实现重试循环。

**小结**
kubernetes会在容器崩溃或者探针失败的时候把容器重启，这项任务是由承载pod节点上的`kubelet`组件负责的，主控节点上的`kubernetes control plane`组件不会参与这个过程。

但如果节点本身崩溃，那么`kubernetes control plane`就必须为所有随节点停止运行的pod创建替代品。它不会直接对pod执行这个操作，这些pod只被kubelet管理，但随着节点停止运行，kubelet也会停止工作。要做到pod在另一个节点上运行，需要使用`ReplicationController`或者其他控制器来实现。

# ReplicationController
`ReplicationController`是一种kubernetes资源，可确保它的pod始终保持运行状态。如果pod由于任何原因在kubernetes集群中消失，那么ReplicationController会注意到缺少了pod并创建替代pod。

`ReplicationController`主要用在创建和管理一个`replicas`副本。这就是它名字的由来。
## 作用过程
`ReplicationController`会持续监控正在运行的pod列表，并保证相应"类型"的pod数目与期望相符。如果运行的pod太少，它将根据模板创建pod；如果运行的pod太多，那么它将删除多余的pod；一般来说，会影响pod运行数量的原因有以下几点：
- 手动创建或删除pod；
- 更改现有pod的"类型"；
- 减少ReplicationController的副本数。

前面用了好几次“类型“这种说法，其实这种说法是不存在的。ReplicationController不是根据pod类型来操作的，而是根据pod是否匹配某个标签选择器。

**控制协调过程**
ReplicationController的工作是保证pod的数量始终与标签选择器的数量相匹配。如果不匹配，ReplicationController则采取适当的操作来协调pod的数量。如下图所示：
![](image/第三章：副本机制和控制器_time_1.png)
上图所示，ReplicationController作用主要靠三个部分：
- label selector(标签选择器)，用于确定ReplicationController作用域中有哪些pod；
- replica count(副本个数)，指定应运行的pod数量；
- pod template(pod模板)，用于指定创建新的pod副本。
这三样都可以随时被修改，但只有修改`replica count`会影响现有pod。

**修改影响**
如果修改了`label selector`，会影响`ReplicationController`对pod的监控列表，会放弃对现有pod的监控，转而去创建具有新标签选择器的pod。

如果修改了`pod template`，不会对现有pod造成影响，只会影响后续创建的新pod。

**使用RS的好处**
`ReplicationController`是kubernetes中一个简单的概念，但却提供了很多强大的功能：
- 保证pod的持续运行，方法是创建新的pod副本；
- 集群节点发生故障时，为故障节点上RS监控的节点创建代替副本；
- 很容易使用RS实现pod的水平伸缩。

## 创建ReplicationController
下面我们通过一个yaml文件来创建一个`ReplicationController`
```yaml
apiVersion: v1
kind: ReplicationController   # 资源类型
metadata:
  name: kubia
spec:
  replicas: 3               # 副本数   
  selector:
    app: kubia              # 监控具有相应标签的pod
  template:                 # 创建pod的模板
    metadata:
      labels:
        app: kubia
    spec:
      containers:
      - name: kubia
        image: luksa/kubia
        ports:
        - containerPort: 8080
```
模板中pod的标签必须和标签选择器匹配，否则`ReplicationController`会无休止的创建新的pod，因为创建新的pod不会使pod的数量接近期望的副本数。为了防止这种情况，API服务会校验`ReplicationController`中的定义，不会接收错误的配置。

也可以不知道标签选择器，这时kubernetes会从模板中自动提取它，这样会使pod更加简洁。
## 使用ReplicationController

